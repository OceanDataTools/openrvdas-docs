<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>logger.utils.cached_data_server API documentation</title>
<meta name="description" content="NOTE: See below for mysterious cache behavior when invoked via
logger_manager.py â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>logger.utils.cached_data_server</code></h1>
</header>
<section id="section-intro">
<p>NOTE: See below for mysterious cache behavior when invoked via
logger_manager.py</p>
<p>Accept data in DASRecord or dict format via the cache_record() method,
then serve it to anyone who connects via a websocket. A
CachedDataServer can be instantiated by running this script from the
command line and providing one or more UDP ports on which to listen
for timestamped text data that it can parse into key:value pairs. It
may also be instantiated as part of a CachedDataWriter that can be
invoked on the command line via the listen.py script of via a
configuration file.</p>
<p>The following direct invocation of this script</p>
<pre><code>    logger/utils/cached_data_server.py       --udp 6225       --port 8766       --back_seconds 480       --v
</code></pre>
<p>says to</p>
<ol>
<li>
<p>Listen on the UDP port specified by &ndash;udp for JSON-encoded,
timestamped, field:value pairs. (See the definition for cache_record(),
below for formats understood.)</p>
</li>
<li>
<p>Store the received data in an in-memory cache, retaining the most
recent 480 seconds for each field.</p>
</li>
<li>
<p>Wait for clients to connect to the websocket at port 8766 and serve
them the requested data. Web clients may issue JSON-encoded
requests of the following formats (see the definition of
serve_requests() for insight):</p>
</li>
</ol>
<pre><code>   {'type':'fields'}   - return a list of fields for which cache has data

   {'type':'subscribe',
    'fields':{'field_1':{'seconds':50},
              'field_2':{'seconds':0},
              'field_3':{'seconds':-1}}}

       - subscribe to updates for field_1, field_2 and field_3. Allowable
         values for 'seconds':

            0  - provide only new values that arrive after subscription
           -1  - provide the most recent value, and then all future new ones
           num - provide num seconds of back data, then all future new ones

         If 'seconds' is missing, use '0' as the default.

   {'type':'ready'}

       - indicate that client is ready to receive the next set of updates
         for subscribed fields.

   {'type':'publish', 'data':{'timestamp':1555468528.452,
                              'fields':{'field_1':'value_1',
                                        'field_2':'value_2'}}}

       - submit new data to the cache (an alternative way to get data
         in without the same record size limits of a UDP packet).
</code></pre>
<p>A CachedDataServer may also be created by invoking the listen.py
script and creating a CachedDataWriter (which is just a wrapper around
CachedDataServer). It may be invoked with the same options, and has
the added benefit that you can have your server take data from a wider
variety of sources (by using a LogfileReader, DatabaseReader,
RedisReader or the like):</p>
<pre><code>    logger/listener/listen.py       --udp 6221,6224       --parse_definition_path local/devices/*.yaml,test/sikuliaq/devices.yaml       --transform_parse       --write_cached_data_writer 8766
</code></pre>
<p>This command
line creates
a CachedDataWriter that
reads timestamped
NMEA sentences, parses them into
field:value pairs, then stores them,
as above, in in-memory cached to be served to connected webclients.</p>
<p>Note that the listen.py script currently provides no way to override
the default values for back_seconds (480) and cleanup (60). But a
contributor who wished could easily add the appropriate flags to the
listen.py script.</p>
<p>Finally, it may be incorporated (again, within its CachedDataWriter
wrapper) into a logger via a configuration file:</p>
<pre><code>    logger/listener/listen.py --config_file data_server_config.yaml
</code></pre>
<p>where data_server_config.yaml contains:</p>
<pre><code>readers:
- class: UDPReader
  kwargs:
    port: 6221
- class: UDPReader
  kwargs:
    port: 6224
transforms:
- class: ParseTransform
writers:
- class: CachedDataWriter
  kwargs:
    back_seconds: 480
    cleanup: 60
    port: 8766
</code></pre>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">#!/usr/bin/env python3
&#34;&#34;&#34;NOTE: See below for mysterious cache behavior when invoked via
logger_manager.py

Accept data in DASRecord or dict format via the cache_record() method,
then serve it to anyone who connects via a websocket. A
CachedDataServer can be instantiated by running this script from the
command line and providing one or more UDP ports on which to listen
for timestamped text data that it can parse into key:value pairs. It
may also be instantiated as part of a CachedDataWriter that can be
invoked on the command line via the listen.py script of via a
configuration file.

The following direct invocation of this script
```
    logger/utils/cached_data_server.py \
      --udp 6225 \
      --port 8766 \
      --back_seconds 480 \
      --v
```
says to

1. Listen on the UDP port specified by --udp for JSON-encoded,
   timestamped, field:value pairs. (See the definition for cache_record(),
   below for formats understood.)

2. Store the received data in an in-memory cache, retaining the most
   recent 480 seconds for each field.

3. Wait for clients to connect to the websocket at port 8766 and serve
   them the requested data. Web clients may issue JSON-encoded
   requests of the following formats (see the definition of
   serve_requests() for insight):
```
   {&#39;type&#39;:&#39;fields&#39;}   - return a list of fields for which cache has data

   {&#39;type&#39;:&#39;subscribe&#39;,
    &#39;fields&#39;:{&#39;field_1&#39;:{&#39;seconds&#39;:50},
              &#39;field_2&#39;:{&#39;seconds&#39;:0},
              &#39;field_3&#39;:{&#39;seconds&#39;:-1}}}

       - subscribe to updates for field_1, field_2 and field_3. Allowable
         values for &#39;seconds&#39;:

            0  - provide only new values that arrive after subscription
           -1  - provide the most recent value, and then all future new ones
           num - provide num seconds of back data, then all future new ones

         If &#39;seconds&#39; is missing, use &#39;0&#39; as the default.

   {&#39;type&#39;:&#39;ready&#39;}

       - indicate that client is ready to receive the next set of updates
         for subscribed fields.

   {&#39;type&#39;:&#39;publish&#39;, &#39;data&#39;:{&#39;timestamp&#39;:1555468528.452,
                              &#39;fields&#39;:{&#39;field_1&#39;:&#39;value_1&#39;,
                                        &#39;field_2&#39;:&#39;value_2&#39;}}}

       - submit new data to the cache (an alternative way to get data
         in without the same record size limits of a UDP packet).
```
A CachedDataServer may also be created by invoking the listen.py
script and creating a CachedDataWriter (which is just a wrapper around
CachedDataServer). It may be invoked with the same options, and has
the added benefit that you can have your server take data from a wider
variety of sources (by using a LogfileReader, DatabaseReader,
RedisReader or the like):
```
    logger/listener/listen.py \
      --udp 6221,6224 \
      --parse_definition_path local/devices/*.yaml,test/sikuliaq/devices.yaml \
      --transform_parse \
      --write_cached_data_writer 8766
```
This command  line creates  a CachedDataWriter that  reads timestamped
NMEA sentences, parses them into  field:value pairs, then stores them,
as above, in in-memory cached to be served to connected webclients.

Note that the listen.py script currently provides no way to override
the default values for back_seconds (480) and cleanup (60). But a
contributor who wished could easily add the appropriate flags to the
listen.py script.

Finally, it may be incorporated (again, within its CachedDataWriter
wrapper) into a logger via a configuration file:
```
    logger/listener/listen.py --config_file data_server_config.yaml
```
where data_server_config.yaml contains:
```
readers:
- class: UDPReader
  kwargs:
    port: 6221
- class: UDPReader
  kwargs:
    port: 6224
transforms:
- class: ParseTransform
writers:
- class: CachedDataWriter
  kwargs:
    back_seconds: 480
    cleanup: 60
    port: 8766
```
&#34;&#34;&#34;
import asyncio
import json
import logging
import pprint
import sys
import threading
import time
import websockets

from os.path import dirname, realpath; sys.path.append(dirname(dirname(dirname(realpath(__file__)))))

from logger.utils.das_record import DASRecord

LOGGING_FORMAT = &#39;%(asctime)-15s %(filename)s:%(lineno)d %(message)s&#39;
LOG_LEVELS = {0:logging.WARNING, 1:logging.INFO, 2:logging.DEBUG}

############################
def cache_record(record, cache, cache_lock):
  &#34;&#34;&#34;Add the passed record to the passed cache.

  Expects passed records to be in one of two formats:

  1) DASRecord

  2) A dict encoding optionally a source data_id and timestamp and a
     mandatory &#39;fields&#39; key of field_name: value pairs. This is the format
     emitted by default by ParseTransform:
```
     {
       &#39;data_id&#39;: ...,    # optional
       &#39;timestamp&#39;: ...,  # optional - use time.time() if missing
       &#39;fields&#39;: {
         field_name: value,
         field_name: value,
         ...
       }
     }
```
  A twist on format (2) is that the values may either be a singleton
  (int, float, string, etc) or a list. If the value is a singleton,
  it is taken at face value. If it is a list, it is assumed to be a
  list of (value, timestamp) tuples, in which case the top-level
  timestamp, if any, is ignored.
```
     {
       &#39;data_id&#39;: ...,  # optional
       &#39;fields&#39;: {
          field_name: [(timestamp, value), (timestamp, value),...],
          field_name: [(timestamp, value), (timestamp, value),...],
          ...
       }
     }
```
  &#34;&#34;&#34;
  logging.debug(&#39;cache_record() received: %s&#39;, record)
  if not record:
    logging.debug(&#39;cache_record() received empty record.&#39;)
    return

  # If we&#39;ve been passed a DASRecord, the field:value pairs are in a
  # field called, uh, &#39;fields&#39;; if we&#39;ve been passed a dict, look
  # for its &#39;fields&#39; key.
  if type(record) is DASRecord:
    record_timestamp = record.timestamp
    fields = record.fields
  elif type(record) is dict:
    record_timestamp = record.get(&#39;timestamp&#39;, time.time())
    fields = record.get(&#39;fields&#39;, None)
    if fields is None:
      logging.error(&#39;Dict record passed to cache_record() has no &#39;
                    &#39;&#34;fields&#34; key, which either means it\&#39;s not a dict &#39;
                    &#39;you should be passing, or it is in the old &#34;field_dict&#34; &#39;
                    &#39;format that assumes key:value pairs are at the top &#39;
                    &#39;level.&#39;)
      logging.error(&#39;The record in question: %s&#39;, str(record))
      return
  else:
    logging.warning(&#39;Received non-DASRecord, non-dict input (type: %s): %s&#39;,
                      type(record), record)
    return

  # Add values from record to cache
  with cache_lock:
    for field, value in fields.items():
      if not field in cache:
        cache[field] = []

      if type(value) is list:
        # Okay, for this field we have a list of values - iterate through
        for val in value:
          # If element in the list is itself a list or a tuple,
          # we&#39;ll assume it&#39;s a (timestamp, value) pair. Otherwise,
          # use the default timestamp of &#39;now&#39;.
          if type(val) in [list, tuple]:
            cache[field].append(val)
          else:
            cache[field].append((record_timestamp, value))
      else:
        # If type(value) is *not* a list, assume it&#39;s the value
        # itself. Add it using the default timestamp.
        cache[field].append((record_timestamp, value))


############################
class WebSocketConnection:
  &#34;&#34;&#34;Handle the websocket connection, serving data as requested.&#34;&#34;&#34;
  ############################
  def __init__(self, websocket, cache, cache_lock, interval):
    self.websocket = websocket
    self.cache = cache
    self.cache_lock = cache_lock
    self.interval = interval
    self.quit_flag = False

  ############################
  def closed(self):
    &#34;&#34;&#34;Has our client closed the connection?&#34;&#34;&#34;
    return self.quit_flag

  ############################
  def quit(self):
    &#34;&#34;&#34;Close the connection from our end and quit.&#34;&#34;&#34;
    logging.info(&#39;WebSocketConnection quit() signaled&#39;)
    self.quit_flag = True

  ############################
  async def send_json_response(self, response, is_error=False):
    logging.debug(&#39;CachedDataServer sending %d bytes&#39;,
                  len(json.dumps(response)))
    await self.websocket.send(json.dumps(response))
    if is_error:
      logging.warning(response)

  ############################
  @asyncio.coroutine
  async def serve_requests(self):
    &#34;&#34;&#34;Wait for requests and serve data, if it exists, from
    cache. Requests are in JSON with request type encoded in
    &#39;request_type&#39; field. Recognized request types are:
    ```
    fields - return a (JSON encoded) list of fields for which cache
        has data.

    publish - look for a field called &#39;data&#39; and expect its value to
        be a dict containing data in one of the formats accepted by
        cache_record().

    subscribe - look for a field called &#39;fields&#39; in the request whose
        value is a dict of the format

          {field_name:{seconds:600}, field_name:{seconds:0},...}

        May also have a field called &#39;interval&#39;, specifying how often
        server should provide updates. Will default to what was
        specified on command line with --interval flag (which itself
        defaults to 1 second intervals).

        Begin serving JSON messages of the format
          {
            field_name: [(timestamp, value), (timestamp, value),...],
            field_name: [(timestamp, value), (timestamp, value),...],
            field_name: [(timestamp, value), (timestamp, value),...],
          }

        Initially provide the number of seconds worth of back data
        requested, and on subsequent calls, return all data that have
        arrived since last call.

        NOTE: if the &#39;seconds&#39; field is -1, server will only ever provide
        the single most recent value for the relevant field.

    ready - client has processed the previous data message and is ready
        for more.
    ```
    &#34;&#34;&#34;
    # A map from field_name:latest_timestamp_sent. If
    # latest_timestamp_sent is -1, then we&#39;ll always send just the
    # most recent value we have for the field, regardless of how many
    # there are, or whether we&#39;ve sent it before.
    field_timestamps = {}
    interval = self.interval # Use the default interval, uh, by default

    while not self.quit_flag:
      now = time.time()
      try:
        logging.debug(&#39;Waiting for client&#39;)
        raw_request = await self.websocket.recv()
        request = json.loads(raw_request)

        # Make sure we&#39;ve received a dict
        if not type(request) is dict:
          await self.send_json_response(
            {&#39;status&#39;:400, &#39;error&#39;:&#39;non-dict request received&#39;},
            is_error=True)

        # Make sure request dict has a &#39;type&#39; field
        elif not &#39;type&#39; in request:
          await self.send_json_response(
            {&#39;status&#39;:400, &#39;error&#39;:&#39;no &#34;type&#34; field found in request&#39;},
            is_error=True)

        # Let&#39;s see what type of request it is

        # Send client a list of the variable names we&#39;re able to serve.
        elif request[&#39;type&#39;] == &#39;fields&#39;:
          logging.debug(&#39;fields request&#39;)
          await self.send_json_response(
            {&#39;type&#39;:&#39;fields&#39;, &#39;status&#39;:200, &#39;data&#39;:list(self.cache.keys())})

        # Client wants to publish to cache and provides a dict of data
        elif request[&#39;type&#39;] == &#39;publish&#39;:
          logging.debug(&#39;publish request&#39;)
          data = request.get(&#39;data&#39;, None)
          if data is  None:
            await self.send_json_response(
              {&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;no data field found in request&#39;},
               is_error=True)
          elif type(data) is not dict:
            await self.send_json_response(
              {&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;request has non-dict data field&#39;},
               is_error=True)
          else:
            cache_record(data, self.cache, self.cache_lock)
            await self.send_json_response({&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:200})

        # Client wants to subscribe, and provides a dict of requested fields
        elif request[&#39;type&#39;] == &#39;subscribe&#39;:
          logging.debug(&#39;subscribe request&#39;)
          # Have they given us a new subscription interval?
          requested_interval = request.get(&#39;interval&#39;, None)
          if requested_interval is not None:
            try:
              interval = float(requested_interval)
            except ValueError:
              await self.send_json_response(
                {&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:400,
                 &#39;error&#39;:&#39;non-numeric interval requested&#39;},
                is_error=True)
              continue

          requested_fields = request.get(&#39;fields&#39;, None)
          if not requested_fields:
            await self.send_json_response(
              {&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;no fields found in subscribe request&#39;},
              is_error=True)
            continue

          # Parse out request field names and number of back seconds
          # requested. Encode that as &#39;last timestamp sent&#39;, unless back
          # seconds == -1. If -1, save it as -1, so that we know we&#39;re
          # always just sending the the most recent field value.
          field_timestamps = {}
          for field_name, field_spec in requested_fields.items():
            if not type(field_spec) is dict:
              back_seconds = 0
            else:
              back_seconds = field_spec.get(&#39;seconds&#39;, 0)
            if back_seconds == -1:
              field_timestamps[field_name] = -1
            else:
              field_timestamps[field_name] = time.time() - back_seconds

          # Let client know request succeeded
          await self.send_json_response({&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:200})

        # Client just letting us know it&#39;s ready for more. If there are
        # fields that have been requested, send along any new data for
        # them.
        elif request[&#39;type&#39;] == &#39;ready&#39;:
          logging.debug(&#39;Websocket got ready...&#39;)
          if not field_timestamps:
            await self.send_json_response(
              {&#39;type&#39;:&#39;ready&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;client ready, but no data requested.&#39;},
              is_error=True)
            continue

          results = {}
          for field_name, latest_timestamp in field_timestamps.items():
            field_cache = self.cache.get(field_name, None)
            if field_cache is None:
              logging.debug(&#39;No cached data for %s&#39;, field_name)
              continue

            # If no data for requested field, skip.
            if not field_cache or not field_cache[-1]:
              continue

            # If special case -1, they want just single most recent
            # value, then future results. Grab last value, then set its
            # timestamp as the last one we&#39;ve seen.
            elif latest_timestamp == -1:
              last_value = field_cache[-1]
              results[field_name] = [ last_value ]
              field_timestamps[field_name] = last_value[0] # ts of last value

            # Otherwise - if no data newer than the latest
            # timestamp we&#39;ve already sent, skip,
            elif not field_cache[-1][0] &gt; latest_timestamp:
              continue

            # Otherwise, copy over records arrived since
            # latest_timestamp and update the latest_timestamp sent
            # (first element of last pair in field_cache).
            else:
              results[field_name] = [pair for pair in field_cache if
                                     pair[0] &gt; latest_timestamp]
              if field_cache:
                field_timestamps[field_name] = field_cache[-1][0]

          logging.debug(&#39;Websocket results: %s...&#39;, str(results)[0:100])

          # Package up what results we have (if any) and send them off
          await self.send_json_response({&#39;type&#39;:&#39;data&#39;, &#39;status&#39;:200,
                                         &#39;data&#39;:results})

          # New results or not, take a nap before trying to fetch more results
          elapsed = time.time() - now
          time_to_sleep = max(0, interval - elapsed)
          logging.debug(&#39;Sleeping %g seconds&#39;, time_to_sleep)
          await asyncio.sleep(time_to_sleep)

        # If unrecognized request type - whine, then iterate
        else:
          await self.send_json_response(
            {&#39;status&#39;:400,
             &#39;error&#39;:&#39;unrecognized request type: %s&#39; % request_type},
              is_error=True)

      # If we got bad input, complain and loop
      except json.JSONDecodeError:
        await self.send_json_response(
          {&#39;status&#39;:400, &#39;error&#39;:&#39;received unparseable JSON&#39;},
          is_error=True)
        logging.warning(&#39;unparseable JSON: %s&#39;, raw_request)

      # If our connection closed, complain and exit gracefully
      except websockets.exceptions.ConnectionClosed:
        logging.info(&#39;Client closed connection&#39;)
        self.quit()

################################################################################
class CachedDataServer:
  &#34;&#34;&#34;Class that caches field:value pairs passed to it in either a
  DASRecord or a simple dict. It also establishes a websocket server
  on the specified port and serves the cached values to clients that
  connect via a websocket.

  The server listens for two types of requests:

  1. If the request is the string &#34;variables&#34;, return a list of the
     names of the variables the server has in cache and is able to
     serve. The server will continue listening for follow up messages,
     most likely this one:

  2. If the request is a python dict, assume it is of the form:
  ```
      {field_1_name: {&#39;seconds&#39;: num_secs},
       field_2_name: {&#39;seconds&#39;: num_secs},
       ...}
  ```
     where seconds is a float representing the number of seconds of
     back data being requested.

     This field dict is passed to serve_fields(), which will to retrieve
     num_secs of back data for each of the specified fields and return it
     as a JSON-encoded dict of the form:
  ```
       {
         field_1_name: [(timestamp, value), (timestamp, value), ...],
         field_2_name: [(timestamp, value), (timestamp, value), ...],
         ...
       }
  ```
  The server will then await a &#34;ready&#34; message from the client, and when
  received, will loop and send a JSON-encoded dict of all the
  (timestamp, value) tuples that have come in since the previous
  request. It will continue this behavior indefinitely, waiting for a
  &#34;ready&#34; request and sending updates.

  &#34;&#34;&#34;

  ############################
  def __init__(self, port, interval=1, event_loop=None):
    self.port = port
    self.interval = interval
    self.cache = {}
    self.cache_lock = threading.Lock()

    # List where we&#39;ll store our websocket connections so that we can
    # keep track of which are still open, and signal them to close
    # when we&#39;re done.
    self._connections = []
    self._connection_lock = threading.Lock()

    # If we&#39;ve received an event loop, use it, otherwise create a new one
    # of our own.
    if not event_loop:
      self.event_loop = asyncio.new_event_loop()
      asyncio.set_event_loop(self.event_loop)
    else:
      self.event_loop = None
      asyncio.set_event_loop(event_loop)

    self.quit_flag = False

    # Fire up the thread that&#39;s going to the websocket server in our
    # event loop. Calling quit() it will close any remaining
    # connections and stop the event loop, terminating the server.
    self.server_thread = threading.Thread(target=self._run_websocket_server,
                                          daemon=True)
    self.server_thread.start()

  ############################
  def __del__(self):
    if self.event_loop:
      self.event_loop.stop()
      self.event_loop.close()

  ############################
  def cache_record(self, record):
    &#34;&#34;&#34;Add the passed record to the cache.&#34;&#34;&#34;
    cache_record(record, self.cache, self.cache_lock)

  ############################
  def cleanup(self, oldest):
    &#34;&#34;&#34;Remove any data from cache with a timestamp older than &#39;oldest&#39;
    seconds, but keep at least one (most recent) value.
    &#34;&#34;&#34;
    logging.debug(&#39;Cleaning up cache&#39;)
    with self.cache_lock:
      for field in self.cache:
        value_list = self.cache[field]
        while value_list and len(value_list) &gt; 1 and value_list[0][0] &lt; oldest:
          value_list.pop(0)

  ############################
  def _run_websocket_server(self):
    &#34;&#34;&#34;Start serving on the specified websocket.
    &#34;&#34;&#34;
    logging.info(&#39;Starting WebSocketServer on port %d&#39;, self.port)
    try:
      self.websocket_server = websockets.serve(
        ws_handler=self._serve_websocket_data,
        host=&#39;&#39;, port=self.port, loop=self.event_loop)

      # If event loop is already running, just add server to task list
      if self.event_loop.is_running():
        asyncio.ensure_future(self.websocket_server, loop=self.event_loop)

      # Otherwise, fire up the event loop now
      else:
        self.event_loop.run_until_complete(self.websocket_server)
        self.event_loop.run_forever()
    except OSError as e:
      logging.fatal(&#39;Failed to open websocket on port %s: %s&#39;,self.port, str(e))
      raise e

  ############################
  def quit(self):
    &#34;&#34;&#34;Exit the loop and shut down all loggers.
    &#34;&#34;&#34;
    self.quit_flag = True

    # Close any connections
    with self._connection_lock:
      for connection in self._connections:
        connection.quit()
    logging.info(&#39;WebSocketServer closed&#39;)

    # Stop the event loop that&#39;s serving connections
    self.event_loop.stop()

    # Wait for thread that&#39;s running the server to finish
    self.server_thread.join()

  ############################
  &#34;&#34;&#34;Top-level coroutine for running CachedDataServer.&#34;&#34;&#34;
  @asyncio.coroutine
  async def _serve_websocket_data(self, websocket, path):
    logging.info(&#39;New data websocket client attached: %s&#39;, path)

    # Here is where we see the anomalous behavior - when constructed
    # directly, self.cache is as it should be: a shared cache. But
    # when invoked indirectly, e.g. as part of a listener via
    #
    #    listener = ListenerFromLoggerConfig(config)
    #    proc = multiprocessing.Process(target=listener.run, daemon=True)
    #    proc.start()
    #
    # then self.cache always appears ins in its initial (empty) state.
    connection = WebSocketConnection(websocket, self.cache, self.cache_lock,
                                     self.interval)

    # Stash the connection so we can tell it to exit when we receive a
    # quit(). But first do some cleanup, getting rid of old
    # connections that have closed.
    with self._connection_lock:
      index = 0
      while index &lt; len(self._connections):
        if self._connections[index].closed():
          logging.debug(&#39;Disposing of closed connection.&#39;)
          self._connections.pop(index)
        else:
          index += 1
      # Now add the new connection
      self._connections.append(connection)

    # If client disconnects, tell connection to quit
    try:
      await connection.serve_requests()
    except websockets.ConnectionClosed:
      logging.warning(&#39;client disconnected&#39;)
    except KeyboardInterrupt:
      logging.warning(&#39;Keyboard Interrupt&#39;)

    connection.quit()
    await websocket.close()

################################################################################
################################################################################
if __name__ == &#39;__main__&#39;:
  import argparse

  from logger.readers.composed_reader import ComposedReader
  from logger.readers.udp_reader import UDPReader
  from logger.transforms.from_json_transform import FromJSONTransform
  from logger.utils import record_parser

  parser = argparse.ArgumentParser()
  parser.add_argument(&#39;--port&#39;, dest=&#39;port&#39;, required=True,
                      action=&#39;store&#39;, type=int,
                      help=&#39;Websocket port on which to serve data&#39;)

  parser.add_argument(&#39;--udp&#39;, dest=&#39;udp&#39;, default=None, action=&#39;store&#39;,
                      help=&#39;Comma-separated list of network ports to listen &#39;
                      &#39;for data on, e.g. 6221,6224. Prefix by group id &#39;
                      &#39;to specify multicast.&#39;)

  parser.add_argument(&#39;--back_seconds&#39;, dest=&#39;back_seconds&#39;, action=&#39;store&#39;,
                      type=float, default=480,
                      help=&#39;Maximum number of seconds of old data to keep &#39;
                      &#39;for serving to new clients.&#39;)

  parser.add_argument(&#39;--cleanup_interval&#39;, dest=&#39;cleanup_interval&#39;,
                      action=&#39;store&#39;, type=float, default=60,
                      help=&#39;How often to clean old data out of the cache.&#39;)

  parser.add_argument(&#39;--interval&#39;, dest=&#39;interval&#39;, action=&#39;store&#39;,
                      type=float, default=1,
                      help=&#39;How many seconds to sleep between successive &#39;
                      &#39;sends of data to clients.&#39;)

  parser.add_argument(&#39;-v&#39;, &#39;--verbosity&#39;, dest=&#39;verbosity&#39;, default=0,
                      action=&#39;count&#39;, help=&#39;Increase output verbosity&#39;)
  args = parser.parse_args()

  # Set logging verbosity
  args.verbosity = min(args.verbosity, max(LOG_LEVELS))
  logging.getLogger().setLevel(LOG_LEVELS[args.verbosity])

  # Only create reader(s) if they&#39;ve given us a network to read from;
  # otherwise, count on data coming from websocket publish
  # connections.
  if args.udp:
    readers = []
    # Readers may either be just a port (to listen for broadcast) or
    # a multicast_group:port to listen for multicast.
    for udp_spec in args.udp.split(&#39;,&#39;):
      group_port = udp_spec.split(&#39;:&#39;)
      port = int(group_port[-1])
      multicast_group = group_port[-2] if len(group_port) == 2 else &#39;&#39;
      readers.append(UDPReader(port=port, source=multicast_group))
    transform = FromJSONTransform()
    reader = ComposedReader(readers=readers, transforms=[transform])

  server = CachedDataServer(args.port, args.interval)

  # Every N seconds, we&#39;re going to detour to clean old data out of cache
  next_cleanup_time = time.time() + args.cleanup_interval

  # Loop, reading data and writing it to the cache
  try:
    while True:
      if args.udp:
        record = reader.read()
        logging.debug(&#39;Got record: %s&#39;, record)

        # If, for some reason, we get empty record try again
        server.cache_record(record)
      else:
        time.sleep(args.interval)

      # Is it time for next cleanup?
      now = time.time()
      if now &gt; next_cleanup_time:
        server.cleanup(now - args.back_seconds)
        next_cleanup_time = now + args.cleanup_interval
  except KeyboardInterrupt:
    logging.warning(&#39;Received KeyboardInterrupt - shutting down&#39;)
    server.quit()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="logger.utils.cached_data_server.cache_record"><code class="name flex">
<span>def <span class="ident">cache_record</span></span>(<span>record, cache, cache_lock)</span>
</code></dt>
<dd>
<section class="desc"><p>Add the passed record to the passed cache.</p>
<p>Expects passed records to be in one of two formats:</p>
<p>1) DASRecord</p>
<p>2) A dict encoding optionally a source data_id and timestamp and a
mandatory 'fields' key of field_name: value pairs. This is the format
emitted by default by ParseTransform:</p>
<pre><code>     {
       'data_id': ...,    # optional
       'timestamp': ...,  # optional - use time.time() if missing
       'fields': {
         field_name: value,
         field_name: value,
         ...
       }
     }
</code></pre>
<p>A twist on format (2) is that the values may either be a singleton
(int, float, string, etc) or a list. If the value is a singleton,
it is taken at face value. If it is a list, it is assumed to be a
list of (value, timestamp) tuples, in which case the top-level
timestamp, if any, is ignored.</p>
<pre><code>     {
       'data_id': ...,  # optional
       'fields': {
          field_name: [(timestamp, value), (timestamp, value),...],
          field_name: [(timestamp, value), (timestamp, value),...],
          ...
       }
     }
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def cache_record(record, cache, cache_lock):
  &#34;&#34;&#34;Add the passed record to the passed cache.

  Expects passed records to be in one of two formats:

  1) DASRecord

  2) A dict encoding optionally a source data_id and timestamp and a
     mandatory &#39;fields&#39; key of field_name: value pairs. This is the format
     emitted by default by ParseTransform:
```
     {
       &#39;data_id&#39;: ...,    # optional
       &#39;timestamp&#39;: ...,  # optional - use time.time() if missing
       &#39;fields&#39;: {
         field_name: value,
         field_name: value,
         ...
       }
     }
```
  A twist on format (2) is that the values may either be a singleton
  (int, float, string, etc) or a list. If the value is a singleton,
  it is taken at face value. If it is a list, it is assumed to be a
  list of (value, timestamp) tuples, in which case the top-level
  timestamp, if any, is ignored.
```
     {
       &#39;data_id&#39;: ...,  # optional
       &#39;fields&#39;: {
          field_name: [(timestamp, value), (timestamp, value),...],
          field_name: [(timestamp, value), (timestamp, value),...],
          ...
       }
     }
```
  &#34;&#34;&#34;
  logging.debug(&#39;cache_record() received: %s&#39;, record)
  if not record:
    logging.debug(&#39;cache_record() received empty record.&#39;)
    return

  # If we&#39;ve been passed a DASRecord, the field:value pairs are in a
  # field called, uh, &#39;fields&#39;; if we&#39;ve been passed a dict, look
  # for its &#39;fields&#39; key.
  if type(record) is DASRecord:
    record_timestamp = record.timestamp
    fields = record.fields
  elif type(record) is dict:
    record_timestamp = record.get(&#39;timestamp&#39;, time.time())
    fields = record.get(&#39;fields&#39;, None)
    if fields is None:
      logging.error(&#39;Dict record passed to cache_record() has no &#39;
                    &#39;&#34;fields&#34; key, which either means it\&#39;s not a dict &#39;
                    &#39;you should be passing, or it is in the old &#34;field_dict&#34; &#39;
                    &#39;format that assumes key:value pairs are at the top &#39;
                    &#39;level.&#39;)
      logging.error(&#39;The record in question: %s&#39;, str(record))
      return
  else:
    logging.warning(&#39;Received non-DASRecord, non-dict input (type: %s): %s&#39;,
                      type(record), record)
    return

  # Add values from record to cache
  with cache_lock:
    for field, value in fields.items():
      if not field in cache:
        cache[field] = []

      if type(value) is list:
        # Okay, for this field we have a list of values - iterate through
        for val in value:
          # If element in the list is itself a list or a tuple,
          # we&#39;ll assume it&#39;s a (timestamp, value) pair. Otherwise,
          # use the default timestamp of &#39;now&#39;.
          if type(val) in [list, tuple]:
            cache[field].append(val)
          else:
            cache[field].append((record_timestamp, value))
      else:
        # If type(value) is *not* a list, assume it&#39;s the value
        # itself. Add it using the default timestamp.
        cache[field].append((record_timestamp, value))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="logger.utils.cached_data_server.CachedDataServer"><code class="flex name class">
<span>class <span class="ident">CachedDataServer</span></span>
<span>(</span><span>port, interval=1, event_loop=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Class that caches field:value pairs passed to it in either a
DASRecord or a simple dict. It also establishes a websocket server
on the specified port and serves the cached values to clients that
connect via a websocket.</p>
<p>The server listens for two types of requests:</p>
<ol>
<li>
<p>If the request is the string "variables", return a list of the
names of the variables the server has in cache and is able to
serve. The server will continue listening for follow up messages,
most likely this one:</p>
</li>
<li>
<p>If the request is a python dict, assume it is of the form:</p>
</li>
</ol>
<pre><code>    {field_1_name: {'seconds': num_secs},
     field_2_name: {'seconds': num_secs},
     ...}
</code></pre>
<p>where seconds is a float representing the number of seconds of
back data being requested.</p>
<p>This field dict is passed to serve_fields(), which will to retrieve
num_secs of back data for each of the specified fields and return it
as a JSON-encoded dict of the form:</p>
<pre><code>     {
       field_1_name: [(timestamp, value), (timestamp, value), ...],
       field_2_name: [(timestamp, value), (timestamp, value), ...],
       ...
     }
</code></pre>
<p>The server will then await a "ready" message from the client, and when
received, will loop and send a JSON-encoded dict of all the
(timestamp, value) tuples that have come in since the previous
request. It will continue this behavior indefinitely, waiting for a
"ready" request and sending updates.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class CachedDataServer:
  &#34;&#34;&#34;Class that caches field:value pairs passed to it in either a
  DASRecord or a simple dict. It also establishes a websocket server
  on the specified port and serves the cached values to clients that
  connect via a websocket.

  The server listens for two types of requests:

  1. If the request is the string &#34;variables&#34;, return a list of the
     names of the variables the server has in cache and is able to
     serve. The server will continue listening for follow up messages,
     most likely this one:

  2. If the request is a python dict, assume it is of the form:
  ```
      {field_1_name: {&#39;seconds&#39;: num_secs},
       field_2_name: {&#39;seconds&#39;: num_secs},
       ...}
  ```
     where seconds is a float representing the number of seconds of
     back data being requested.

     This field dict is passed to serve_fields(), which will to retrieve
     num_secs of back data for each of the specified fields and return it
     as a JSON-encoded dict of the form:
  ```
       {
         field_1_name: [(timestamp, value), (timestamp, value), ...],
         field_2_name: [(timestamp, value), (timestamp, value), ...],
         ...
       }
  ```
  The server will then await a &#34;ready&#34; message from the client, and when
  received, will loop and send a JSON-encoded dict of all the
  (timestamp, value) tuples that have come in since the previous
  request. It will continue this behavior indefinitely, waiting for a
  &#34;ready&#34; request and sending updates.

  &#34;&#34;&#34;

  ############################
  def __init__(self, port, interval=1, event_loop=None):
    self.port = port
    self.interval = interval
    self.cache = {}
    self.cache_lock = threading.Lock()

    # List where we&#39;ll store our websocket connections so that we can
    # keep track of which are still open, and signal them to close
    # when we&#39;re done.
    self._connections = []
    self._connection_lock = threading.Lock()

    # If we&#39;ve received an event loop, use it, otherwise create a new one
    # of our own.
    if not event_loop:
      self.event_loop = asyncio.new_event_loop()
      asyncio.set_event_loop(self.event_loop)
    else:
      self.event_loop = None
      asyncio.set_event_loop(event_loop)

    self.quit_flag = False

    # Fire up the thread that&#39;s going to the websocket server in our
    # event loop. Calling quit() it will close any remaining
    # connections and stop the event loop, terminating the server.
    self.server_thread = threading.Thread(target=self._run_websocket_server,
                                          daemon=True)
    self.server_thread.start()

  ############################
  def __del__(self):
    if self.event_loop:
      self.event_loop.stop()
      self.event_loop.close()

  ############################
  def cache_record(self, record):
    &#34;&#34;&#34;Add the passed record to the cache.&#34;&#34;&#34;
    cache_record(record, self.cache, self.cache_lock)

  ############################
  def cleanup(self, oldest):
    &#34;&#34;&#34;Remove any data from cache with a timestamp older than &#39;oldest&#39;
    seconds, but keep at least one (most recent) value.
    &#34;&#34;&#34;
    logging.debug(&#39;Cleaning up cache&#39;)
    with self.cache_lock:
      for field in self.cache:
        value_list = self.cache[field]
        while value_list and len(value_list) &gt; 1 and value_list[0][0] &lt; oldest:
          value_list.pop(0)

  ############################
  def _run_websocket_server(self):
    &#34;&#34;&#34;Start serving on the specified websocket.
    &#34;&#34;&#34;
    logging.info(&#39;Starting WebSocketServer on port %d&#39;, self.port)
    try:
      self.websocket_server = websockets.serve(
        ws_handler=self._serve_websocket_data,
        host=&#39;&#39;, port=self.port, loop=self.event_loop)

      # If event loop is already running, just add server to task list
      if self.event_loop.is_running():
        asyncio.ensure_future(self.websocket_server, loop=self.event_loop)

      # Otherwise, fire up the event loop now
      else:
        self.event_loop.run_until_complete(self.websocket_server)
        self.event_loop.run_forever()
    except OSError as e:
      logging.fatal(&#39;Failed to open websocket on port %s: %s&#39;,self.port, str(e))
      raise e

  ############################
  def quit(self):
    &#34;&#34;&#34;Exit the loop and shut down all loggers.
    &#34;&#34;&#34;
    self.quit_flag = True

    # Close any connections
    with self._connection_lock:
      for connection in self._connections:
        connection.quit()
    logging.info(&#39;WebSocketServer closed&#39;)

    # Stop the event loop that&#39;s serving connections
    self.event_loop.stop()

    # Wait for thread that&#39;s running the server to finish
    self.server_thread.join()

  ############################
  &#34;&#34;&#34;Top-level coroutine for running CachedDataServer.&#34;&#34;&#34;
  @asyncio.coroutine
  async def _serve_websocket_data(self, websocket, path):
    logging.info(&#39;New data websocket client attached: %s&#39;, path)

    # Here is where we see the anomalous behavior - when constructed
    # directly, self.cache is as it should be: a shared cache. But
    # when invoked indirectly, e.g. as part of a listener via
    #
    #    listener = ListenerFromLoggerConfig(config)
    #    proc = multiprocessing.Process(target=listener.run, daemon=True)
    #    proc.start()
    #
    # then self.cache always appears ins in its initial (empty) state.
    connection = WebSocketConnection(websocket, self.cache, self.cache_lock,
                                     self.interval)

    # Stash the connection so we can tell it to exit when we receive a
    # quit(). But first do some cleanup, getting rid of old
    # connections that have closed.
    with self._connection_lock:
      index = 0
      while index &lt; len(self._connections):
        if self._connections[index].closed():
          logging.debug(&#39;Disposing of closed connection.&#39;)
          self._connections.pop(index)
        else:
          index += 1
      # Now add the new connection
      self._connections.append(connection)

    # If client disconnects, tell connection to quit
    try:
      await connection.serve_requests()
    except websockets.ConnectionClosed:
      logging.warning(&#39;client disconnected&#39;)
    except KeyboardInterrupt:
      logging.warning(&#39;Keyboard Interrupt&#39;)

    connection.quit()
    await websocket.close()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="logger.utils.cached_data_server.CachedDataServer.cache_record"><code class="name flex">
<span>def <span class="ident">cache_record</span></span>(<span>self, record)</span>
</code></dt>
<dd>
<section class="desc"><p>Add the passed record to the cache.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def cache_record(self, record):
  &#34;&#34;&#34;Add the passed record to the cache.&#34;&#34;&#34;
  cache_record(record, self.cache, self.cache_lock)</code></pre>
</details>
</dd>
<dt id="logger.utils.cached_data_server.CachedDataServer.cleanup"><code class="name flex">
<span>def <span class="ident">cleanup</span></span>(<span>self, oldest)</span>
</code></dt>
<dd>
<section class="desc"><p>Remove any data from cache with a timestamp older than 'oldest'
seconds, but keep at least one (most recent) value.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def cleanup(self, oldest):
  &#34;&#34;&#34;Remove any data from cache with a timestamp older than &#39;oldest&#39;
  seconds, but keep at least one (most recent) value.
  &#34;&#34;&#34;
  logging.debug(&#39;Cleaning up cache&#39;)
  with self.cache_lock:
    for field in self.cache:
      value_list = self.cache[field]
      while value_list and len(value_list) &gt; 1 and value_list[0][0] &lt; oldest:
        value_list.pop(0)</code></pre>
</details>
</dd>
<dt id="logger.utils.cached_data_server.CachedDataServer.quit"><code class="name flex">
<span>def <span class="ident">quit</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Exit the loop and shut down all loggers.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def quit(self):
  &#34;&#34;&#34;Exit the loop and shut down all loggers.
  &#34;&#34;&#34;
  self.quit_flag = True

  # Close any connections
  with self._connection_lock:
    for connection in self._connections:
      connection.quit()
  logging.info(&#39;WebSocketServer closed&#39;)

  # Stop the event loop that&#39;s serving connections
  self.event_loop.stop()

  # Wait for thread that&#39;s running the server to finish
  self.server_thread.join()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="logger.utils.cached_data_server.WebSocketConnection"><code class="flex name class">
<span>class <span class="ident">WebSocketConnection</span></span>
<span>(</span><span>websocket, cache, cache_lock, interval)</span>
</code></dt>
<dd>
<section class="desc"><p>Handle the websocket connection, serving data as requested.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class WebSocketConnection:
  &#34;&#34;&#34;Handle the websocket connection, serving data as requested.&#34;&#34;&#34;
  ############################
  def __init__(self, websocket, cache, cache_lock, interval):
    self.websocket = websocket
    self.cache = cache
    self.cache_lock = cache_lock
    self.interval = interval
    self.quit_flag = False

  ############################
  def closed(self):
    &#34;&#34;&#34;Has our client closed the connection?&#34;&#34;&#34;
    return self.quit_flag

  ############################
  def quit(self):
    &#34;&#34;&#34;Close the connection from our end and quit.&#34;&#34;&#34;
    logging.info(&#39;WebSocketConnection quit() signaled&#39;)
    self.quit_flag = True

  ############################
  async def send_json_response(self, response, is_error=False):
    logging.debug(&#39;CachedDataServer sending %d bytes&#39;,
                  len(json.dumps(response)))
    await self.websocket.send(json.dumps(response))
    if is_error:
      logging.warning(response)

  ############################
  @asyncio.coroutine
  async def serve_requests(self):
    &#34;&#34;&#34;Wait for requests and serve data, if it exists, from
    cache. Requests are in JSON with request type encoded in
    &#39;request_type&#39; field. Recognized request types are:
    ```
    fields - return a (JSON encoded) list of fields for which cache
        has data.

    publish - look for a field called &#39;data&#39; and expect its value to
        be a dict containing data in one of the formats accepted by
        cache_record().

    subscribe - look for a field called &#39;fields&#39; in the request whose
        value is a dict of the format

          {field_name:{seconds:600}, field_name:{seconds:0},...}

        May also have a field called &#39;interval&#39;, specifying how often
        server should provide updates. Will default to what was
        specified on command line with --interval flag (which itself
        defaults to 1 second intervals).

        Begin serving JSON messages of the format
          {
            field_name: [(timestamp, value), (timestamp, value),...],
            field_name: [(timestamp, value), (timestamp, value),...],
            field_name: [(timestamp, value), (timestamp, value),...],
          }

        Initially provide the number of seconds worth of back data
        requested, and on subsequent calls, return all data that have
        arrived since last call.

        NOTE: if the &#39;seconds&#39; field is -1, server will only ever provide
        the single most recent value for the relevant field.

    ready - client has processed the previous data message and is ready
        for more.
    ```
    &#34;&#34;&#34;
    # A map from field_name:latest_timestamp_sent. If
    # latest_timestamp_sent is -1, then we&#39;ll always send just the
    # most recent value we have for the field, regardless of how many
    # there are, or whether we&#39;ve sent it before.
    field_timestamps = {}
    interval = self.interval # Use the default interval, uh, by default

    while not self.quit_flag:
      now = time.time()
      try:
        logging.debug(&#39;Waiting for client&#39;)
        raw_request = await self.websocket.recv()
        request = json.loads(raw_request)

        # Make sure we&#39;ve received a dict
        if not type(request) is dict:
          await self.send_json_response(
            {&#39;status&#39;:400, &#39;error&#39;:&#39;non-dict request received&#39;},
            is_error=True)

        # Make sure request dict has a &#39;type&#39; field
        elif not &#39;type&#39; in request:
          await self.send_json_response(
            {&#39;status&#39;:400, &#39;error&#39;:&#39;no &#34;type&#34; field found in request&#39;},
            is_error=True)

        # Let&#39;s see what type of request it is

        # Send client a list of the variable names we&#39;re able to serve.
        elif request[&#39;type&#39;] == &#39;fields&#39;:
          logging.debug(&#39;fields request&#39;)
          await self.send_json_response(
            {&#39;type&#39;:&#39;fields&#39;, &#39;status&#39;:200, &#39;data&#39;:list(self.cache.keys())})

        # Client wants to publish to cache and provides a dict of data
        elif request[&#39;type&#39;] == &#39;publish&#39;:
          logging.debug(&#39;publish request&#39;)
          data = request.get(&#39;data&#39;, None)
          if data is  None:
            await self.send_json_response(
              {&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;no data field found in request&#39;},
               is_error=True)
          elif type(data) is not dict:
            await self.send_json_response(
              {&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;request has non-dict data field&#39;},
               is_error=True)
          else:
            cache_record(data, self.cache, self.cache_lock)
            await self.send_json_response({&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:200})

        # Client wants to subscribe, and provides a dict of requested fields
        elif request[&#39;type&#39;] == &#39;subscribe&#39;:
          logging.debug(&#39;subscribe request&#39;)
          # Have they given us a new subscription interval?
          requested_interval = request.get(&#39;interval&#39;, None)
          if requested_interval is not None:
            try:
              interval = float(requested_interval)
            except ValueError:
              await self.send_json_response(
                {&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:400,
                 &#39;error&#39;:&#39;non-numeric interval requested&#39;},
                is_error=True)
              continue

          requested_fields = request.get(&#39;fields&#39;, None)
          if not requested_fields:
            await self.send_json_response(
              {&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;no fields found in subscribe request&#39;},
              is_error=True)
            continue

          # Parse out request field names and number of back seconds
          # requested. Encode that as &#39;last timestamp sent&#39;, unless back
          # seconds == -1. If -1, save it as -1, so that we know we&#39;re
          # always just sending the the most recent field value.
          field_timestamps = {}
          for field_name, field_spec in requested_fields.items():
            if not type(field_spec) is dict:
              back_seconds = 0
            else:
              back_seconds = field_spec.get(&#39;seconds&#39;, 0)
            if back_seconds == -1:
              field_timestamps[field_name] = -1
            else:
              field_timestamps[field_name] = time.time() - back_seconds

          # Let client know request succeeded
          await self.send_json_response({&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:200})

        # Client just letting us know it&#39;s ready for more. If there are
        # fields that have been requested, send along any new data for
        # them.
        elif request[&#39;type&#39;] == &#39;ready&#39;:
          logging.debug(&#39;Websocket got ready...&#39;)
          if not field_timestamps:
            await self.send_json_response(
              {&#39;type&#39;:&#39;ready&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;client ready, but no data requested.&#39;},
              is_error=True)
            continue

          results = {}
          for field_name, latest_timestamp in field_timestamps.items():
            field_cache = self.cache.get(field_name, None)
            if field_cache is None:
              logging.debug(&#39;No cached data for %s&#39;, field_name)
              continue

            # If no data for requested field, skip.
            if not field_cache or not field_cache[-1]:
              continue

            # If special case -1, they want just single most recent
            # value, then future results. Grab last value, then set its
            # timestamp as the last one we&#39;ve seen.
            elif latest_timestamp == -1:
              last_value = field_cache[-1]
              results[field_name] = [ last_value ]
              field_timestamps[field_name] = last_value[0] # ts of last value

            # Otherwise - if no data newer than the latest
            # timestamp we&#39;ve already sent, skip,
            elif not field_cache[-1][0] &gt; latest_timestamp:
              continue

            # Otherwise, copy over records arrived since
            # latest_timestamp and update the latest_timestamp sent
            # (first element of last pair in field_cache).
            else:
              results[field_name] = [pair for pair in field_cache if
                                     pair[0] &gt; latest_timestamp]
              if field_cache:
                field_timestamps[field_name] = field_cache[-1][0]

          logging.debug(&#39;Websocket results: %s...&#39;, str(results)[0:100])

          # Package up what results we have (if any) and send them off
          await self.send_json_response({&#39;type&#39;:&#39;data&#39;, &#39;status&#39;:200,
                                         &#39;data&#39;:results})

          # New results or not, take a nap before trying to fetch more results
          elapsed = time.time() - now
          time_to_sleep = max(0, interval - elapsed)
          logging.debug(&#39;Sleeping %g seconds&#39;, time_to_sleep)
          await asyncio.sleep(time_to_sleep)

        # If unrecognized request type - whine, then iterate
        else:
          await self.send_json_response(
            {&#39;status&#39;:400,
             &#39;error&#39;:&#39;unrecognized request type: %s&#39; % request_type},
              is_error=True)

      # If we got bad input, complain and loop
      except json.JSONDecodeError:
        await self.send_json_response(
          {&#39;status&#39;:400, &#39;error&#39;:&#39;received unparseable JSON&#39;},
          is_error=True)
        logging.warning(&#39;unparseable JSON: %s&#39;, raw_request)

      # If our connection closed, complain and exit gracefully
      except websockets.exceptions.ConnectionClosed:
        logging.info(&#39;Client closed connection&#39;)
        self.quit()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="logger.utils.cached_data_server.WebSocketConnection.closed"><code class="name flex">
<span>def <span class="ident">closed</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Has our client closed the connection?</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def closed(self):
  &#34;&#34;&#34;Has our client closed the connection?&#34;&#34;&#34;
  return self.quit_flag</code></pre>
</details>
</dd>
<dt id="logger.utils.cached_data_server.WebSocketConnection.quit"><code class="name flex">
<span>def <span class="ident">quit</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Close the connection from our end and quit.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def quit(self):
  &#34;&#34;&#34;Close the connection from our end and quit.&#34;&#34;&#34;
  logging.info(&#39;WebSocketConnection quit() signaled&#39;)
  self.quit_flag = True</code></pre>
</details>
</dd>
<dt id="logger.utils.cached_data_server.WebSocketConnection.send_json_response"><code class="name flex">
<span>async def <span class="ident">send_json_response</span></span>(<span>self, response, is_error=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">async def send_json_response(self, response, is_error=False):
  logging.debug(&#39;CachedDataServer sending %d bytes&#39;,
                len(json.dumps(response)))
  await self.websocket.send(json.dumps(response))
  if is_error:
    logging.warning(response)</code></pre>
</details>
</dd>
<dt id="logger.utils.cached_data_server.WebSocketConnection.serve_requests"><code class="name flex">
<span>async def <span class="ident">serve_requests</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Wait for requests and serve data, if it exists, from
cache. Requests are in JSON with request type encoded in
'request_type' field. Recognized request types are:</p>
<pre><code>fields - return a (JSON encoded) list of fields for which cache
    has data.

publish - look for a field called 'data' and expect its value to
    be a dict containing data in one of the formats accepted by
    cache_record().

subscribe - look for a field called 'fields' in the request whose
    value is a dict of the format

      {field_name:{seconds:600}, field_name:{seconds:0},...}

    May also have a field called 'interval', specifying how often
    server should provide updates. Will default to what was
    specified on command line with --interval flag (which itself
    defaults to 1 second intervals).

    Begin serving JSON messages of the format
      {
        field_name: [(timestamp, value), (timestamp, value),...],
        field_name: [(timestamp, value), (timestamp, value),...],
        field_name: [(timestamp, value), (timestamp, value),...],
      }

    Initially provide the number of seconds worth of back data
    requested, and on subsequent calls, return all data that have
    arrived since last call.

    NOTE: if the 'seconds' field is -1, server will only ever provide
    the single most recent value for the relevant field.

ready - client has processed the previous data message and is ready
    for more.
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@asyncio.coroutine
async def serve_requests(self):
  &#34;&#34;&#34;Wait for requests and serve data, if it exists, from
  cache. Requests are in JSON with request type encoded in
  &#39;request_type&#39; field. Recognized request types are:
  ```
  fields - return a (JSON encoded) list of fields for which cache
      has data.

  publish - look for a field called &#39;data&#39; and expect its value to
      be a dict containing data in one of the formats accepted by
      cache_record().

  subscribe - look for a field called &#39;fields&#39; in the request whose
      value is a dict of the format

        {field_name:{seconds:600}, field_name:{seconds:0},...}

      May also have a field called &#39;interval&#39;, specifying how often
      server should provide updates. Will default to what was
      specified on command line with --interval flag (which itself
      defaults to 1 second intervals).

      Begin serving JSON messages of the format
        {
          field_name: [(timestamp, value), (timestamp, value),...],
          field_name: [(timestamp, value), (timestamp, value),...],
          field_name: [(timestamp, value), (timestamp, value),...],
        }

      Initially provide the number of seconds worth of back data
      requested, and on subsequent calls, return all data that have
      arrived since last call.

      NOTE: if the &#39;seconds&#39; field is -1, server will only ever provide
      the single most recent value for the relevant field.

  ready - client has processed the previous data message and is ready
      for more.
  ```
  &#34;&#34;&#34;
  # A map from field_name:latest_timestamp_sent. If
  # latest_timestamp_sent is -1, then we&#39;ll always send just the
  # most recent value we have for the field, regardless of how many
  # there are, or whether we&#39;ve sent it before.
  field_timestamps = {}
  interval = self.interval # Use the default interval, uh, by default

  while not self.quit_flag:
    now = time.time()
    try:
      logging.debug(&#39;Waiting for client&#39;)
      raw_request = await self.websocket.recv()
      request = json.loads(raw_request)

      # Make sure we&#39;ve received a dict
      if not type(request) is dict:
        await self.send_json_response(
          {&#39;status&#39;:400, &#39;error&#39;:&#39;non-dict request received&#39;},
          is_error=True)

      # Make sure request dict has a &#39;type&#39; field
      elif not &#39;type&#39; in request:
        await self.send_json_response(
          {&#39;status&#39;:400, &#39;error&#39;:&#39;no &#34;type&#34; field found in request&#39;},
          is_error=True)

      # Let&#39;s see what type of request it is

      # Send client a list of the variable names we&#39;re able to serve.
      elif request[&#39;type&#39;] == &#39;fields&#39;:
        logging.debug(&#39;fields request&#39;)
        await self.send_json_response(
          {&#39;type&#39;:&#39;fields&#39;, &#39;status&#39;:200, &#39;data&#39;:list(self.cache.keys())})

      # Client wants to publish to cache and provides a dict of data
      elif request[&#39;type&#39;] == &#39;publish&#39;:
        logging.debug(&#39;publish request&#39;)
        data = request.get(&#39;data&#39;, None)
        if data is  None:
          await self.send_json_response(
            {&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:400,
             &#39;error&#39;:&#39;no data field found in request&#39;},
             is_error=True)
        elif type(data) is not dict:
          await self.send_json_response(
            {&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:400,
             &#39;error&#39;:&#39;request has non-dict data field&#39;},
             is_error=True)
        else:
          cache_record(data, self.cache, self.cache_lock)
          await self.send_json_response({&#39;type&#39;:&#39;publish&#39;, &#39;status&#39;:200})

      # Client wants to subscribe, and provides a dict of requested fields
      elif request[&#39;type&#39;] == &#39;subscribe&#39;:
        logging.debug(&#39;subscribe request&#39;)
        # Have they given us a new subscription interval?
        requested_interval = request.get(&#39;interval&#39;, None)
        if requested_interval is not None:
          try:
            interval = float(requested_interval)
          except ValueError:
            await self.send_json_response(
              {&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:400,
               &#39;error&#39;:&#39;non-numeric interval requested&#39;},
              is_error=True)
            continue

        requested_fields = request.get(&#39;fields&#39;, None)
        if not requested_fields:
          await self.send_json_response(
            {&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:400,
             &#39;error&#39;:&#39;no fields found in subscribe request&#39;},
            is_error=True)
          continue

        # Parse out request field names and number of back seconds
        # requested. Encode that as &#39;last timestamp sent&#39;, unless back
        # seconds == -1. If -1, save it as -1, so that we know we&#39;re
        # always just sending the the most recent field value.
        field_timestamps = {}
        for field_name, field_spec in requested_fields.items():
          if not type(field_spec) is dict:
            back_seconds = 0
          else:
            back_seconds = field_spec.get(&#39;seconds&#39;, 0)
          if back_seconds == -1:
            field_timestamps[field_name] = -1
          else:
            field_timestamps[field_name] = time.time() - back_seconds

        # Let client know request succeeded
        await self.send_json_response({&#39;type&#39;:&#39;subscribe&#39;, &#39;status&#39;:200})

      # Client just letting us know it&#39;s ready for more. If there are
      # fields that have been requested, send along any new data for
      # them.
      elif request[&#39;type&#39;] == &#39;ready&#39;:
        logging.debug(&#39;Websocket got ready...&#39;)
        if not field_timestamps:
          await self.send_json_response(
            {&#39;type&#39;:&#39;ready&#39;, &#39;status&#39;:400,
             &#39;error&#39;:&#39;client ready, but no data requested.&#39;},
            is_error=True)
          continue

        results = {}
        for field_name, latest_timestamp in field_timestamps.items():
          field_cache = self.cache.get(field_name, None)
          if field_cache is None:
            logging.debug(&#39;No cached data for %s&#39;, field_name)
            continue

          # If no data for requested field, skip.
          if not field_cache or not field_cache[-1]:
            continue

          # If special case -1, they want just single most recent
          # value, then future results. Grab last value, then set its
          # timestamp as the last one we&#39;ve seen.
          elif latest_timestamp == -1:
            last_value = field_cache[-1]
            results[field_name] = [ last_value ]
            field_timestamps[field_name] = last_value[0] # ts of last value

          # Otherwise - if no data newer than the latest
          # timestamp we&#39;ve already sent, skip,
          elif not field_cache[-1][0] &gt; latest_timestamp:
            continue

          # Otherwise, copy over records arrived since
          # latest_timestamp and update the latest_timestamp sent
          # (first element of last pair in field_cache).
          else:
            results[field_name] = [pair for pair in field_cache if
                                   pair[0] &gt; latest_timestamp]
            if field_cache:
              field_timestamps[field_name] = field_cache[-1][0]

        logging.debug(&#39;Websocket results: %s...&#39;, str(results)[0:100])

        # Package up what results we have (if any) and send them off
        await self.send_json_response({&#39;type&#39;:&#39;data&#39;, &#39;status&#39;:200,
                                       &#39;data&#39;:results})

        # New results or not, take a nap before trying to fetch more results
        elapsed = time.time() - now
        time_to_sleep = max(0, interval - elapsed)
        logging.debug(&#39;Sleeping %g seconds&#39;, time_to_sleep)
        await asyncio.sleep(time_to_sleep)

      # If unrecognized request type - whine, then iterate
      else:
        await self.send_json_response(
          {&#39;status&#39;:400,
           &#39;error&#39;:&#39;unrecognized request type: %s&#39; % request_type},
            is_error=True)

    # If we got bad input, complain and loop
    except json.JSONDecodeError:
      await self.send_json_response(
        {&#39;status&#39;:400, &#39;error&#39;:&#39;received unparseable JSON&#39;},
        is_error=True)
      logging.warning(&#39;unparseable JSON: %s&#39;, raw_request)

    # If our connection closed, complain and exit gracefully
    except websockets.exceptions.ConnectionClosed:
      logging.info(&#39;Client closed connection&#39;)
      self.quit()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="logger.utils" href="index.html">logger.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="logger.utils.cached_data_server.cache_record" href="#logger.utils.cached_data_server.cache_record">cache_record</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="logger.utils.cached_data_server.CachedDataServer" href="#logger.utils.cached_data_server.CachedDataServer">CachedDataServer</a></code></h4>
<ul class="">
<li><code><a title="logger.utils.cached_data_server.CachedDataServer.cache_record" href="#logger.utils.cached_data_server.CachedDataServer.cache_record">cache_record</a></code></li>
<li><code><a title="logger.utils.cached_data_server.CachedDataServer.cleanup" href="#logger.utils.cached_data_server.CachedDataServer.cleanup">cleanup</a></code></li>
<li><code><a title="logger.utils.cached_data_server.CachedDataServer.quit" href="#logger.utils.cached_data_server.CachedDataServer.quit">quit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="logger.utils.cached_data_server.WebSocketConnection" href="#logger.utils.cached_data_server.WebSocketConnection">WebSocketConnection</a></code></h4>
<ul class="">
<li><code><a title="logger.utils.cached_data_server.WebSocketConnection.closed" href="#logger.utils.cached_data_server.WebSocketConnection.closed">closed</a></code></li>
<li><code><a title="logger.utils.cached_data_server.WebSocketConnection.quit" href="#logger.utils.cached_data_server.WebSocketConnection.quit">quit</a></code></li>
<li><code><a title="logger.utils.cached_data_server.WebSocketConnection.send_json_response" href="#logger.utils.cached_data_server.WebSocketConnection.send_json_response">send_json_response</a></code></li>
<li><code><a title="logger.utils.cached_data_server.WebSocketConnection.serve_requests" href="#logger.utils.cached_data_server.WebSocketConnection.serve_requests">serve_requests</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>